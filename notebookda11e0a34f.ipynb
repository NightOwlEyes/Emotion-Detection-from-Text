{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12005954,"sourceType":"datasetVersion","datasetId":7552835}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall fastai -y","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-05-31T15:36:55.184688Z","iopub.execute_input":"2025-05-31T15:36:55.184918Z","iopub.status.idle":"2025-05-31T15:36:57.942548Z","shell.execute_reply.started":"2025-05-31T15:36:55.184889Z","shell.execute_reply":"2025-05-31T15:36:57.941757Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: fastai 2.7.19\nUninstalling fastai-2.7.19:\n  Successfully uninstalled fastai-2.7.19\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install protobuf==3.20.*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:37:26.225810Z","iopub.execute_input":"2025-05-31T15:37:26.226089Z","iopub.status.idle":"2025-05-31T15:37:29.312840Z","shell.execute_reply.started":"2025-05-31T15:37:26.226050Z","shell.execute_reply":"2025-05-31T15:37:29.312131Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.11/dist-packages (3.20.3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install \"torch>=1.7.0\"\n!pip install \"transformers>=4.5.0\"\n!pip install \"pandas>=1.1.0\"\n!pip install \"numpy>=1.19.0\"\n!pip install \"scikit-learn==1.4.2\"\n!pip install \"matplotlib>=3.3.0\"\n!pip install \"seaborn>=0.11.0\"\n!pip install \"tqdm>=4.50.0\"\n!pip install \"imbalanced-learn==0.12.0\"\n!pip install \"emoji>=2.14.1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:42:21.212466Z","iopub.execute_input":"2025-05-31T15:42:21.213283Z","iopub.status.idle":"2025-05-31T15:42:51.054725Z","shell.execute_reply.started":"2025-05-31T15:42:21.213248Z","shell.execute_reply":"2025-05-31T15:42:51.053824Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0) (3.0.2)\nRequirement already satisfied: transformers>=4.5.0 in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.5.0) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.5.0) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.5.0) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.5.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.5.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.5.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.5.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.5.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.5.0) (2024.2.0)\nRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.1.0) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.0) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=1.1.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=1.1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=1.1.0) (2024.2.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0) (2024.2.0)\nRequirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.11/dist-packages (1.4.2)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.3.0) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.3.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.3.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib>=3.3.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib>=3.3.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib>=3.3.0) (2024.2.0)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.11.0) (1.26.4)\nRequirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.11.0) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.11.0) (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn>=0.11.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn>=0.11.0) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.24.0,>=1.17->seaborn>=0.11.0) (2024.2.0)\nRequirement already satisfied: tqdm>=4.50.0 in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: imbalanced-learn==0.12.0 in /usr/local/lib/python3.11/dist-packages (0.12.0)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.12.0) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.12.0) (1.15.2)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.12.0) (1.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.12.0) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.12.0) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.12.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.12.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.12.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->imbalanced-learn==0.12.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->imbalanced-learn==0.12.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->imbalanced-learn==0.12.0) (2024.2.0)\nRequirement already satisfied: emoji>=2.14.1 in /usr/local/lib/python3.11/dist-packages (2.14.1)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nprint(torch.version.cuda)  # CUDA version PyTorch Ä‘ang dÃ¹ng\nprint(torch.backends.cudnn.version())  # cuDNN version\nprint(torch.cuda.is_available())  # Kiá»ƒm tra Ä‘Ã£ nháº­n GPU chÆ°a\nprint(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:42:56.914111Z","iopub.execute_input":"2025-05-31T15:42:56.914666Z","iopub.status.idle":"2025-05-31T15:43:00.351718Z","shell.execute_reply.started":"2025-05-31T15:42:56.914633Z","shell.execute_reply":"2025-05-31T15:43:00.350996Z"}},"outputs":[{"name":"stdout","text":"12.4\n90100\nTrue\nTesla T4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n chá»©a dataset\nimport os\n\nfor root, dirs, files in os.walk('/kaggle/input'):\n    print(f'ðŸ“‚ {root}')\n    for file in files:\n        print(f'  â””â”€â”€ {file}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:49:49.257216Z","iopub.execute_input":"2025-05-31T15:49:49.257811Z","iopub.status.idle":"2025-05-31T15:49:49.272035Z","shell.execute_reply.started":"2025-05-31T15:49:49.257785Z","shell.execute_reply":"2025-05-31T15:49:49.271490Z"}},"outputs":[{"name":"stdout","text":"ðŸ“‚ /kaggle/input\nðŸ“‚ /kaggle/input/uit-vsmec\n  â””â”€â”€ VnEmoLex.csv\n  â””â”€â”€ valid.csv\n  â””â”€â”€ train.csv\n  â””â”€â”€ test.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell: Ghi file config.py Ä‘á»ƒ import á»Ÿ cÃ¡c cell sau\nwith open(\"config.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write('''# Cáº¥u hÃ¬nh tham sá»‘ cho mÃ´ hÃ¬nh nháº­n dáº¡ng cáº£m xÃºc vÄƒn báº£n\n\n# Cáº¥u hÃ¬nh dá»¯ liá»‡u\nDATA_CONFIG = {\n    'train_path': '/kaggle/input/uit-vsmec/train.csv',\n    'valid_path': '/kaggle/input/uit-vsmec/valid.csv',\n    'test_path': '/kaggle/input/uit-vsmec/test.csv',\n    'vnemolex_path': '/kaggle/input/uit-vsmec/VnEmoLex.csv',\n    'max_len': 128,  # Äá»™ dÃ i tá»‘i Ä‘a cá»§a vÄƒn báº£n\n}\n\n# Cáº¥u hÃ¬nh huáº¥n luyá»‡n\nTRAINING_CONFIG = {\n    'batch_size': 32,\n    'epochs': 20,\n    'learning_rate': 2e-5,\n    'warmup_steps': 0,\n    'weight_decay': 0.01,\n    'dropout_rate': 0.3,\n    'early_stopping_patience': 3,  # Sá»‘ epoch chá» Ä‘á»£i trÆ°á»›c khi dá»«ng sá»›m\n}\n\n# Cáº¥u hÃ¬nh mÃ´ hÃ¬nh\nMODEL_CONFIG = {\n    'bert_model_name': 'uitnlp/CafeBERT',\n    'hidden_size': 512,  # KÃ­ch thÆ°á»›c lá»›p áº©n\n    'num_classes': 6,  # Sá»‘ lÆ°á»£ng lá»›p cáº£m xÃºc (Ä‘Ã£ loáº¡i bá» nhÃ£n Other)\n}\n\n# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n\nPATH_CONFIG = {\n    'model_dir': 'models',\n    'best_model_path': 'models/best_model.pt',\n    'logs_dir': 'logs',\n    'results_dir': 'results',\n}\n\n# Ãnh xáº¡ nhÃ£n cáº£m xÃºc\nEMOTION_MAPPING = {\n    'Anger': 0,\n    'Disgust': 1,\n    'Fear': 2,\n    'Enjoyment': 3,\n    'Sadness': 4,\n    'Surprise': 5\n}\n\n# Ãnh xáº¡ ngÆ°á»£c láº¡i tá»« sá»‘ sang nhÃ£n cáº£m xÃºc\nREVERSE_EMOTION_MAPPING = {v: k for k, v in EMOTION_MAPPING.items()}\n\n# Danh sÃ¡ch cáº£m xÃºc trong tá»« Ä‘iá»ƒn VnEmoLex\nVNEMOLEX_EMOTIONS = ['Anger', 'Disgust', 'Fear', 'Enjoyment', 'Sadness', 'Surprise']\n''')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:47:42.511153Z","iopub.execute_input":"2025-05-31T15:47:42.511413Z","iopub.status.idle":"2025-05-31T15:47:42.516249Z","shell.execute_reply.started":"2025-05-31T15:47:42.511394Z","shell.execute_reply":"2025-05-31T15:47:42.515699Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport emoji\nimport json\nimport time\nimport argparse\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\nimport sys\nif 'config' in sys.modules:\n    del sys.modules['config']\nfrom config import DATA_CONFIG, TRAINING_CONFIG, MODEL_CONFIG, PATH_CONFIG, EMOTION_MAPPING, REVERSE_EMOTION_MAPPING, VNEMOLEX_EMOTIONS\n\n# Táº¡o thÆ° má»¥c Ä‘á»ƒ lÆ°u káº¿t quáº£\nos.makedirs(PATH_CONFIG['results_dir'], exist_ok=True)\nos.makedirs(PATH_CONFIG['model_dir'], exist_ok=True)\nos.makedirs(PATH_CONFIG['logs_dir'], exist_ok=True)\n\n# Cáº¥u hÃ¬nh thiáº¿t bá»‹\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Sá»­ dá»¥ng thiáº¿t bá»‹: {device}\")\n\n# HÃ m tiá»n xá»­ lÃ½ vÄƒn báº£n\ndef preprocess_text(text):\n    text = text.strip().lower()                      # Chuyá»ƒn vá» chá»¯ thÆ°á»ng\n    text = re.sub(r'\\s+', ' ', text)                 # XÃ³a khoáº£ng tráº¯ng thá»«a\n    text = re.sub(r'\\d+', '', text)                  # Loáº¡i bá» sá»‘\n    # Giá»¯ láº¡i emoji\n    emojis = ''.join(c for c in text if c in emoji.EMOJI_DATA)\n    # Loáº¡i kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, khoáº£ng tráº¯ng, dáº¥u cÃ¢u nháº¹\n    text = re.sub(r'[^\\w\\s\\.,!?]', '', text)\n    # GhÃ©p láº¡i vá»›i emoji náº¿u cáº§n\n    return text.strip() + ' ' + emojis if emojis else text.strip()\n\n# LÆ°u thÃ´ng tin tiá»n xá»­ lÃ½\ndef save_preprocessing_info():\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'preprocessing_info.txt'), 'w', encoding='utf-8') as f:\n        f.write(\"Quy trÃ¬nh tiá»n xá»­ lÃ½ vÄƒn báº£n:\\n\")\n        f.write(\"1. Chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t sá»­ dá»¥ng underthesea\\n\")\n        f.write(\"2. Chuyá»ƒn vá» chá»¯ thÆ°á»ng\\n\")\n        f.write(\"3. XÃ³a khoáº£ng tráº¯ng thá»«a\\n\")\n        f.write(\"4. Giá»¯ nguyÃªn emoji vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t\\n\")\n\n# Äá»c dá»¯ liá»‡u\ndef load_data():\n    train_df = pd.read_csv(DATA_CONFIG['train_path'])\n    valid_df = pd.read_csv(DATA_CONFIG['valid_path'])\n    test_df = pd.read_csv(DATA_CONFIG['test_path'])\n    \n    # XÃ³a hÃ ng cÃ³ giÃ¡ trá»‹ NaN\n    train_df = train_df.dropna()\n    valid_df = valid_df.dropna()\n    test_df = test_df.dropna()\n    \n    # Loáº¡i bá» cÃ¡c máº«u cÃ³ nhÃ£n 'Other'\n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p huáº¥n luyá»‡n trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': {len(train_df)}\")\n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm Ä‘á»‹nh trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': {len(valid_df)}\")\n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm tra trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': {len(test_df)}\")\n    \n    train_df = train_df[train_df['Emotion'] != 'Other']\n    valid_df = valid_df[valid_df['Emotion'] != 'Other']\n    test_df = test_df[test_df['Emotion'] != 'Other']\n    \n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p huáº¥n luyá»‡n sau khi loáº¡i bá» nhÃ£n 'Other': {len(train_df)}\")\n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm Ä‘á»‹nh sau khi loáº¡i bá» nhÃ£n 'Other': {len(valid_df)}\")\n    print(f\"Sá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm tra sau khi loáº¡i bá» nhÃ£n 'Other': {len(test_df)}\")\n    \n    # Tiá»n xá»­ lÃ½ vÄƒn báº£n\n    train_df['Sentence'] = train_df['Sentence'].apply(preprocess_text)\n    valid_df['Sentence'] = valid_df['Sentence'].apply(preprocess_text)\n    test_df['Sentence'] = test_df['Sentence'].apply(preprocess_text)\n    \n    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½\n    train_df.to_csv(os.path.join(PATH_CONFIG['logs_dir'], 'preprocessed_train.csv'), index=False)\n    valid_df.to_csv(os.path.join(PATH_CONFIG['logs_dir'], 'preprocessed_valid.csv'), index=False)\n    test_df.to_csv(os.path.join(PATH_CONFIG['logs_dir'], 'preprocessed_test.csv'), index=False)\n    \n    # PhÃ¢n tÃ­ch phÃ¢n phá»‘i nhÃ£n\n    analyze_data_distribution(train_df, valid_df, test_df)\n    \n    return train_df, valid_df, test_df\n\n# PhÃ¢n tÃ­ch phÃ¢n phá»‘i dá»¯ liá»‡u\ndef analyze_data_distribution(train_df, valid_df, test_df):\n    # Äáº¿m sá»‘ lÆ°á»£ng máº«u cho má»—i cáº£m xÃºc\n    train_counts = train_df['Emotion'].value_counts()\n    valid_counts = valid_df['Emotion'].value_counts()\n    test_counts = test_df['Emotion'].value_counts()\n    \n    # LÆ°u thÃ´ng tin phÃ¢n phá»‘i\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'data_distribution.txt'), 'w', encoding='utf-8') as f:\n        f.write(\"PhÃ¢n phá»‘i dá»¯ liá»‡u:\\n\")\n        f.write(f\"Tá»•ng sá»‘ máº«u huáº¥n luyá»‡n: {len(train_df)}\\n\")\n        f.write(f\"Tá»•ng sá»‘ máº«u kiá»ƒm Ä‘á»‹nh: {len(valid_df)}\\n\")\n        f.write(f\"Tá»•ng sá»‘ máº«u kiá»ƒm tra: {len(test_df)}\\n\\n\")\n        \n        f.write(\"PhÃ¢n phá»‘i nhÃ£n trong táº­p huáº¥n luyá»‡n:\\n\")\n        for emotion, count in train_counts.items():\n            f.write(f\"{emotion}: {count} ({count/len(train_df)*100:.2f}%)\\n\")\n        \n        f.write(\"\\nPhÃ¢n phá»‘i nhÃ£n trong táº­p kiá»ƒm Ä‘á»‹nh:\\n\")\n        for emotion, count in valid_counts.items():\n            f.write(f\"{emotion}: {count} ({count/len(valid_df)*100:.2f}%)\\n\")\n        \n        f.write(\"\\nPhÃ¢n phá»‘i nhÃ£n trong táº­p kiá»ƒm tra:\\n\")\n        for emotion, count in test_counts.items():\n            f.write(f\"{emotion}: {count} ({count/len(test_df)*100:.2f}%)\\n\")\n    \n    # Váº½ biá»ƒu Ä‘á»“ phÃ¢n phá»‘i\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    train_counts.plot(kind='bar', color='blue')\n    plt.title('PhÃ¢n phá»‘i nhÃ£n - Táº­p huáº¥n luyá»‡n')\n    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')\n    plt.xticks(rotation=45)\n    \n    plt.subplot(1, 3, 2)\n    valid_counts.plot(kind='bar', color='green')\n    plt.title('PhÃ¢n phá»‘i nhÃ£n - Táº­p kiá»ƒm Ä‘á»‹nh')\n    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')\n    plt.xticks(rotation=45)\n    \n    plt.subplot(1, 3, 3)\n    test_counts.plot(kind='bar', color='red')\n    plt.title('PhÃ¢n phá»‘i nhÃ£n - Táº­p kiá»ƒm tra')\n    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(PATH_CONFIG['results_dir'], 'data_distribution.png'))\n    plt.close()\n    \n    return train_counts\n\n# Äá»c tá»« Ä‘iá»ƒn cáº£m xÃºc VnEmoLex\ndef load_vnemolex():\n    vnemolex_df = pd.read_csv(DATA_CONFIG['vnemolex_path'])\n    \n    # Táº¡o tá»« Ä‘iá»ƒn cáº£m xÃºc\n    emotion_dict = {}\n    for _, row in vnemolex_df.iterrows():\n        word = row['Vietnamese']\n        emotions = {}\n        for emotion in VNEMOLEX_EMOTIONS:\n            if emotion in row and row[emotion] == 1:\n                emotions[emotion] = 1\n        if emotions:\n            emotion_dict[word] = emotions\n    \n    # LÆ°u thÃ´ng tin tá»« Ä‘iá»ƒn\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'vnemolex_info.txt'), 'w', encoding='utf-8') as f:\n        f.write(f\"Tá»•ng sá»‘ tá»« trong tá»« Ä‘iá»ƒn VnEmoLex: {len(emotion_dict)}\\n\")\n        emotion_counts = {emotion: 0 for emotion in VNEMOLEX_EMOTIONS}\n        for word, emotions in emotion_dict.items():\n            for emotion in emotions:\n                emotion_counts[emotion] += 1\n        f.write(\"Sá»‘ lÆ°á»£ng tá»« cho má»—i cáº£m xÃºc:\\n\")\n        for emotion, count in emotion_counts.items():\n            f.write(f\"{emotion}: {count}\\n\")\n    \n    return emotion_dict\n\n# Táº¡o Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\ndef extract_lexicon_features(text, emotion_dict):\n    words = text.split()\n    features = {emotion: 0 for emotion in VNEMOLEX_EMOTIONS}\n    \n    for word in words:\n        if word in emotion_dict:\n            for emotion, value in emotion_dict[word].items():\n                features[emotion] += value\n    \n    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n    total = sum(features.values())\n    if total > 0:\n        for emotion in features:\n            features[emotion] /= total\n    \n    return list(features.values())\n\n# Táº¡o dataset PyTorch\nclass EmotionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, emotion_dict):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.emotion_dict = emotion_dict\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        text = self.data.iloc[index]['Sentence']\n        emotion = self.data.iloc[index]['Emotion']\n        \n        # Tokenize vÄƒn báº£n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\n        lexicon_features = extract_lexicon_features(text, self.emotion_dict)\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'lexicon_features': torch.tensor(lexicon_features, dtype=torch.float),\n            'label': torch.tensor(EMOTION_MAPPING[emotion], dtype=torch.long)\n        }\n\n# MÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc\nclass EmotionClassifier(nn.Module):\n    def __init__(self, bert_model, num_classes=MODEL_CONFIG['num_classes']):\n        super(EmotionClassifier, self).__init__()\n        self.bert = bert_model\n        self.dropout = nn.Dropout(TRAINING_CONFIG['dropout_rate'])\n        \n        # KÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh BERT\n        self.bert_output_dim = self.bert.config.hidden_size\n        \n        # Sá»‘ Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\n        self.lexicon_features_dim = len(VNEMOLEX_EMOTIONS)\n        \n        # Lá»›p káº¿t há»£p Ä‘áº·c trÆ°ng BERT vÃ  Ä‘áº·c trÆ°ng tá»« Ä‘iá»ƒn\n        self.feature_combiner = nn.Linear(self.bert_output_dim + self.lexicon_features_dim, MODEL_CONFIG['hidden_size'])\n        \n        # Lá»›p phÃ¢n loáº¡i\n        self.classifier = nn.Linear(MODEL_CONFIG['hidden_size'], num_classes)\n        \n        # HÃ m kÃ­ch hoáº¡t\n        self.relu = nn.ReLU()\n    \n    def forward(self, input_ids, attention_mask, lexicon_features):\n        # Äáº§u ra tá»« mÃ´ hÃ¬nh BERT\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        \n        # Káº¿t há»£p Ä‘áº·c trÆ°ng BERT vÃ  Ä‘áº·c trÆ°ng tá»« Ä‘iá»ƒn\n        combined_features = torch.cat((pooled_output, lexicon_features), dim=1)\n        combined_features = self.feature_combiner(combined_features)\n        combined_features = self.relu(combined_features)\n        combined_features = self.dropout(combined_features)\n        \n        # PhÃ¢n loáº¡i\n        logits = self.classifier(combined_features)\n        \n        return logits\n\n# TÃ­nh trá»ng sá»‘ cho tá»«ng lá»›p dá»±a trÃªn táº§n suáº¥t xuáº¥t hiá»‡n\ndef calculate_class_weights(train_df):\n    class_counts = train_df['Emotion'].value_counts().to_dict()\n    total_samples = len(train_df)\n    \n    # TÃ­nh trá»ng sá»‘ nghá»‹ch Ä‘áº£o táº§n suáº¥t lá»›p\n    class_weights = {emotion: total_samples / (len(class_counts) * count) \n                    for emotion, count in class_counts.items()}\n    \n    # Chuyá»ƒn Ä‘á»•i thÃ nh tensor\n    weights = torch.FloatTensor([class_weights[emotion] for emotion in REVERSE_EMOTION_MAPPING.values()])\n    \n    # LÆ°u thÃ´ng tin trá»ng sá»‘\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'class_weights.txt'), 'w', encoding='utf-8') as f:\n        f.write(\"Trá»ng sá»‘ cho tá»«ng lá»›p cáº£m xÃºc:\\n\")\n        for emotion, weight in class_weights.items():\n            f.write(f\"{emotion}: {weight:.4f}\\n\")\n    \n    return weights\n\n# Táº¡o sampler cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng\ndef create_weighted_sampler(train_df):\n    # Láº¥y nhÃ£n\n    train_labels = [EMOTION_MAPPING[emotion] for emotion in train_df['Emotion']]\n    \n    # Äáº¿m sá»‘ lÆ°á»£ng máº«u cho má»—i lá»›p\n    class_counts = Counter(train_labels)\n    \n    # TÃ­nh trá»ng sá»‘ cho tá»«ng máº«u\n    weights = [1.0 / class_counts[label] for label in train_labels]\n    \n    # Táº¡o sampler\n    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n    \n    return sampler\n\n# Ãp dá»¥ng oversampling cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng\ndef apply_oversampling(train_df):\n    # TÃ¡ch features vÃ  labels\n    X = train_df.index.values.reshape(-1, 1)  # Sá»­ dá»¥ng chá»‰ sá»‘ lÃ m Ä‘áº·c trÆ°ng\n    y = train_df['Emotion'].values\n    \n    # Ãp dá»¥ng RandomOverSampler\n    ros = RandomOverSampler(random_state=42)\n    X_resampled, y_resampled = ros.fit_resample(X, y)\n    \n    # Táº¡o DataFrame má»›i vá»›i dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c oversampling\n    resampled_indices = X_resampled.flatten()\n    oversampled_df = train_df.iloc[resampled_indices].copy()\n    oversampled_df['Emotion'] = y_resampled\n    \n    # LÆ°u thÃ´ng tin oversampling\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'oversampling_info.txt'), 'w', encoding='utf-8') as f:\n        f.write(\"ThÃ´ng tin oversampling:\\n\")\n        f.write(f\"Sá»‘ lÆ°á»£ng máº«u trÆ°á»›c khi oversampling: {len(train_df)}\\n\")\n        f.write(f\"Sá»‘ lÆ°á»£ng máº«u sau khi oversampling: {len(oversampled_df)}\\n\\n\")\n        \n        original_counts = train_df['Emotion'].value_counts()\n        resampled_counts = oversampled_df['Emotion'].value_counts()\n        \n        f.write(\"PhÃ¢n phá»‘i nhÃ£n trÆ°á»›c khi oversampling:\\n\")\n        for emotion, count in original_counts.items():\n            f.write(f\"{emotion}: {count} ({count/len(train_df)*100:.2f}%)\\n\")\n        \n        f.write(\"\\nPhÃ¢n phá»‘i nhÃ£n sau khi oversampling:\\n\")\n        for emotion, count in resampled_counts.items():\n            f.write(f\"{emotion}: {count} ({count/len(oversampled_df)*100:.2f}%)\\n\")\n    \n    # Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh phÃ¢n phá»‘i trÆ°á»›c vÃ  sau khi oversampling\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    original_counts.plot(kind='bar', color='blue')\n    plt.title('PhÃ¢n phá»‘i nhÃ£n trÆ°á»›c khi oversampling')\n    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')\n    plt.xticks(rotation=45)\n    \n    plt.subplot(1, 2, 2)\n    resampled_counts.plot(kind='bar', color='green')\n    plt.title('PhÃ¢n phá»‘i nhÃ£n sau khi oversampling')\n    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(PATH_CONFIG['results_dir'], 'oversampling_distribution.png'))\n    plt.close()\n    \n    return oversampled_df\n\n# HÃ m huáº¥n luyá»‡n mÃ´ hÃ¬nh\ndef train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs, class_weights=None):\n    # HÃ m máº¥t mÃ¡t vá»›i trá»ng sá»‘ lá»›p (náº¿u cÃ³)\n    if class_weights is not None:\n        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n        print(\"Sá»­ dá»¥ng trá»ng sá»‘ lá»›p cho hÃ m máº¥t mÃ¡t\")\n    else:\n        criterion = nn.CrossEntropyLoss()\n    \n    # LÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_macro_f1': [],\n        'val_weighted_f1': [],\n        'val_class_f1': []\n    }\n    \n    # LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t\n    best_val_f1 = 0\n    patience_counter = 0\n    \n    # Báº¯t Ä‘áº§u huáº¥n luyá»‡n\n    for epoch in range(epochs):\n        print(f'Epoch {epoch+1}/{epochs}')\n        print('-' * 10)\n        \n        # ===== Huáº¥n luyá»‡n =====\n        model.train()\n        train_loss = 0\n        \n        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n        for batch in progress_bar:\n            # ÄÆ°a dá»¯ liá»‡u lÃªn thiáº¿t bá»‹\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            lexicon_features = batch['lexicon_features'].to(device)\n            labels = batch['label'].to(device)\n            \n            # XÃ³a gradient\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask, lexicon_features)\n            \n            # TÃ­nh máº¥t mÃ¡t\n            loss = criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            # Backward pass\n            loss.backward()\n            \n            # Cáº­p nháº­t tham sá»‘\n            optimizer.step()\n            scheduler.step()\n            \n            # Cáº­p nháº­t thanh tiáº¿n trÃ¬nh\n            progress_bar.set_postfix({'loss': loss.item()})\n        \n        # TÃ­nh máº¥t mÃ¡t trung bÃ¬nh trÃªn táº­p huáº¥n luyá»‡n\n        avg_train_loss = train_loss / len(train_dataloader)\n        history['train_loss'].append(avg_train_loss)\n        \n        # ===== ÄÃ¡nh giÃ¡ =====\n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_labels = []\n        \n        with torch.no_grad():\n            progress_bar = tqdm(val_dataloader, desc=\"Validation\")\n            for batch in progress_bar:\n                # ÄÆ°a dá»¯ liá»‡u lÃªn thiáº¿t bá»‹\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                lexicon_features = batch['lexicon_features'].to(device)\n                labels = batch['label'].to(device)\n                \n                # Forward pass\n                outputs = model(input_ids, attention_mask, lexicon_features)\n                \n                # TÃ­nh máº¥t mÃ¡t\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                # Láº¥y dá»± Ä‘oÃ¡n\n                _, preds = torch.max(outputs, dim=1)\n                \n                # LÆ°u dá»± Ä‘oÃ¡n vÃ  nhÃ£n\n                val_preds.extend(preds.cpu().tolist())\n                val_labels.extend(labels.cpu().tolist())\n                \n                # Cáº­p nháº­t thanh tiáº¿n trÃ¬nh\n                progress_bar.set_postfix({'loss': loss.item()})\n        \n        # TÃ­nh máº¥t mÃ¡t trung bÃ¬nh trÃªn táº­p kiá»ƒm Ä‘á»‹nh\n        avg_val_loss = val_loss / len(val_dataloader)\n        history['val_loss'].append(avg_val_loss)\n        \n        # TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (loáº¡i bá» Accuracy vÃ¬ bá»™ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng)\n        val_macro_f1 = f1_score(val_labels, val_preds, average='macro')\n        val_weighted_f1 = f1_score(val_labels, val_preds, average='weighted')\n        val_class_f1 = f1_score(val_labels, val_preds, average=None)\n        \n        history['val_macro_f1'].append(val_macro_f1)\n        history['val_weighted_f1'].append(val_weighted_f1)\n        history['val_class_f1'].append(val_class_f1.tolist())\n        \n        print(f'Train Loss: {avg_train_loss:.4f}')\n        print(f'Val Loss: {avg_val_loss:.4f}')\n        print(f'Val Macro F1: {val_macro_f1:.4f}')\n        print(f'Val Weighted F1: {val_weighted_f1:.4f}')\n        print('\\nF1-score cho tá»«ng lá»›p:')\n        class_names = [REVERSE_EMOTION_MAPPING[i] for i in range(len(REVERSE_EMOTION_MAPPING))]\n        for i, class_name in enumerate(class_names):\n            print(f'{class_name}: {val_class_f1[i]:.4f}')\n        \n        # LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t dá»±a trÃªn Macro F1\n        if val_macro_f1 > best_val_f1:\n            best_val_f1 = val_macro_f1\n            torch.save(model.state_dict(), os.path.join(PATH_CONFIG['model_dir'], 'best_model.pt'))\n            print(\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t!\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n sau má»™t sá»‘ epoch\n        if patience_counter >= TRAINING_CONFIG['early_stopping_patience']:\n            print(f\"Dá»«ng sá»›m sau {epoch+1} epochs vÃ¬ khÃ´ng cáº£i thiá»‡n!\")\n            break\n    \n    # Váº½ biá»ƒu Ä‘á»“ lá»‹ch sá»­ huáº¥n luyá»‡n\n    plot_training_history(history)\n    \n    # LÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'training_history.json'), 'w') as f:\n        json.dump(history, f)\n    \n    return history\n\n# Váº½ biá»ƒu Ä‘á»“ lá»‹ch sá»­ huáº¥n luyá»‡n\ndef plot_training_history(history):\n    plt.figure(figsize=(15, 10))\n    \n    # Váº½ biá»ƒu Ä‘á»“ máº¥t mÃ¡t\n    plt.subplot(2, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Máº¥t mÃ¡t qua cÃ¡c epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Váº½ biá»ƒu Ä‘á»“ F1-score cho tá»«ng lá»›p\n    plt.subplot(2, 2, 2)\n    class_names = [REVERSE_EMOTION_MAPPING[i] for i in range(len(REVERSE_EMOTION_MAPPING))]\n    for i, class_name in enumerate(class_names):\n        class_f1_values = [epoch_f1[i] for epoch_f1 in history['val_class_f1']]\n        plt.plot(class_f1_values, label=class_name)\n    plt.title('F1-score cho tá»«ng lá»›p qua cÃ¡c epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1-score')\n    plt.legend()\n    \n    # Váº½ biá»ƒu Ä‘á»“ F1-score\n    plt.subplot(2, 2, 3)\n    plt.plot(history['val_macro_f1'], label='Macro F1')\n    plt.plot(history['val_weighted_f1'], label='Weighted F1')\n    plt.title('F1-score qua cÃ¡c epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1-score')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(PATH_CONFIG['results_dir'], 'training_history.png'))\n    plt.close()\n\n# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra\ndef evaluate_model(model, test_dataloader, device):\n    # Chuyá»ƒn mÃ´ hÃ¬nh sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡\n    model.eval()\n    \n    # LÆ°u dá»± Ä‘oÃ¡n vÃ  nhÃ£n\n    all_preds = []\n    all_labels = []\n    \n    # KhÃ´ng tÃ­nh gradient\n    with torch.no_grad():\n        progress_bar = tqdm(test_dataloader, desc=\"Testing\")\n        for batch in progress_bar:\n            # ÄÆ°a dá»¯ liá»‡u lÃªn thiáº¿t bá»‹\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            lexicon_features = batch['lexicon_features'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask, lexicon_features)\n            \n            # Láº¥y dá»± Ä‘oÃ¡n\n            _, preds = torch.max(outputs, dim=1)\n            \n            # LÆ°u dá»± Ä‘oÃ¡n vÃ  nhÃ£n\n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    \n    # TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (loáº¡i bá» Accuracy vÃ¬ bá»™ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n    class_f1 = f1_score(all_labels, all_preds, average=None)\n    \n    # TÃ­nh ma tráº­n nháº§m láº«n\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n    with open(os.path.join(PATH_CONFIG['results_dir'], 'evaluation_results.txt'), 'w') as f:\n        f.write(f\"Macro F1-score: {macro_f1:.4f}\\n\")\n        f.write(f\"Weighted F1-score: {weighted_f1:.4f}\\n\\n\")\n        f.write(\"F1-score cho tá»«ng lá»›p:\\n\")\n        class_names = [REVERSE_EMOTION_MAPPING[i] for i in range(len(REVERSE_EMOTION_MAPPING))]\n        for i, class_name in enumerate(class_names):\n            f.write(f\"{class_name}: {class_f1[i]:.4f}\\n\")\n    \n    # Váº½ ma tráº­n nháº§m láº«n\n    plot_confusion_matrix(cm)\n    \n    return macro_f1, weighted_f1, class_f1, cm\n\n# Váº½ ma tráº­n nháº§m láº«n\ndef plot_confusion_matrix(cm):\n    # Danh sÃ¡ch nhÃ£n cáº£m xÃºc\n    labels = list(EMOTION_MAPPING.keys())\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.title('Ma tráº­n nháº§m láº«n')\n    plt.xlabel('Dá»± Ä‘oÃ¡n')\n    plt.ylabel('Thá»±c táº¿')\n    plt.tight_layout()\n    plt.savefig(os.path.join(PATH_CONFIG['results_dir'], 'confusion_matrix.png'))\n    plt.close()\n\n# HÃ m dá»± Ä‘oÃ¡n cáº£m xÃºc cho vÄƒn báº£n má»›i\ndef predict_emotion(text, model, tokenizer, emotion_dict, device):\n    # Tiá»n xá»­ lÃ½ vÄƒn báº£n sá»­ dá»¥ng underthesea\n    text = preprocess_text(text)\n    \n    # Tokenize vÄƒn báº£n\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=DATA_CONFIG['max_len'],\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\n    lexicon_features = extract_lexicon_features(text, emotion_dict)\n    \n    # ÄÆ°a dá»¯ liá»‡u lÃªn thiáº¿t bá»‹\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    lexicon_features = torch.tensor([lexicon_features], dtype=torch.float).to(device)\n    \n    # Dá»± Ä‘oÃ¡n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask, lexicon_features)\n        _, preds = torch.max(outputs, dim=1)\n    \n    return REVERSE_EMOTION_MAPPING[preds.item()]\n\n# HÃ m chÃ­nh\ndef main():\n    # PhÃ¢n tÃ­ch tham sá»‘ dÃ²ng lá»‡nh\n    parser = argparse.ArgumentParser(description='Huáº¥n luyá»‡n mÃ´ hÃ¬nh nháº­n dáº¡ng cáº£m xÃºc vÄƒn báº£n')\n    parser.add_argument('--imbalance_method', type=str, default='weighted_sampler', \n                        choices=['none', 'class_weight', 'weighted_sampler', 'oversampling'],\n                        help='PhÆ°Æ¡ng phÃ¡p xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng')\n    args = parser.parse_args([])\n    \n    # Báº¯t Ä‘áº§u Ä‘o thá»i gian\n    start_time = time.time()\n    \n    # LÆ°u thÃ´ng tin tiá»n xá»­ lÃ½\n    save_preprocessing_info()\n    \n    # Äá»c dá»¯ liá»‡u\n    print(\"Äang Ä‘á»c dá»¯ liá»‡u...\")\n    train_df, valid_df, test_df = load_data()\n    \n    # Xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng\n    class_weights = None\n    sampler = None\n    \n    if args.imbalance_method == 'class_weight':\n        print(\"Ãp dá»¥ng trá»ng sá»‘ lá»›p cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng...\")\n        class_weights = calculate_class_weights(train_df)\n    elif args.imbalance_method == 'weighted_sampler':\n        print(\"Ãp dá»¥ng weighted sampler cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng...\")\n        sampler = create_weighted_sampler(train_df)\n    elif args.imbalance_method == 'oversampling':\n        print(\"Ãp dá»¥ng oversampling cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng...\")\n        train_df = apply_oversampling(train_df)\n    else:\n        print(\"KhÃ´ng Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng\")\n    \n    # Äá»c tá»« Ä‘iá»ƒn cáº£m xÃºc\n    print(\"Äang Ä‘á»c tá»« Ä‘iá»ƒn cáº£m xÃºc...\")\n    emotion_dict = load_vnemolex()\n    \n    # Táº£i mÃ´ hÃ¬nh vÃ  tokenizer\n    print(\"Äang táº£i mÃ´ hÃ¬nh CafeBERT...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['bert_model_name'])\n    bert_model = AutoModel.from_pretrained(MODEL_CONFIG['bert_model_name'])\n    \n    # Táº¡o dataset\n    print(\"Äang táº¡o dataset...\")\n    train_dataset = EmotionDataset(train_df, tokenizer, DATA_CONFIG['max_len'], emotion_dict)\n    valid_dataset = EmotionDataset(valid_df, tokenizer, DATA_CONFIG['max_len'], emotion_dict)\n    test_dataset = EmotionDataset(test_df, tokenizer, DATA_CONFIG['max_len'], emotion_dict)\n    \n    # Táº¡o dataloader\n    if args.imbalance_method == 'weighted_sampler' and sampler is not None:\n        train_dataloader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'], sampler=sampler)\n        print(\"Sá»­ dá»¥ng weighted sampler cho train dataloader\")\n    else:\n        train_dataloader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=True)\n    \n    valid_dataloader = DataLoader(valid_dataset, batch_size=TRAINING_CONFIG['batch_size'])\n    test_dataloader = DataLoader(test_dataset, batch_size=TRAINING_CONFIG['batch_size'])\n    \n    # Táº¡o mÃ´ hÃ¬nh\n    print(\"Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh...\")\n    model = EmotionClassifier(bert_model)\n    model.to(device)\n    \n    # Táº¡o optimizer vÃ  scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=TRAINING_CONFIG['learning_rate'], \n                           weight_decay=TRAINING_CONFIG['weight_decay'])\n    total_steps = len(train_dataloader) * TRAINING_CONFIG['epochs']\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=TRAINING_CONFIG['warmup_steps'],\n        num_training_steps=total_steps\n    )\n    \n    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n    print(\"Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh...\")\n    history = train_model(model, train_dataloader, valid_dataloader, optimizer, scheduler, \n                         device, TRAINING_CONFIG['epochs'], class_weights)\n    \n    # Táº£i mÃ´ hÃ¬nh tá»‘t nháº¥t\n    print(\"Äang táº£i mÃ´ hÃ¬nh tá»‘t nháº¥t...\")\n    model.load_state_dict(torch.load(os.path.join(PATH_CONFIG['model_dir'], 'best_model.pt')))\n    \n    # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra\n    print(\"Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh...\")\n    macro_f1, weighted_f1, class_f1, cm = evaluate_model(model, test_dataloader, device)\n    \n    print(f\"Macro F1-score: {macro_f1:.4f}\")\n    print(f\"Weighted F1-score: {weighted_f1:.4f}\")\n    print(\"\\nF1-score cho tá»«ng lá»›p:\")\n    class_names = [REVERSE_EMOTION_MAPPING[i] for i in range(len(REVERSE_EMOTION_MAPPING))]\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name}: {class_f1[i]:.4f}\")\n    \n    # Káº¿t thÃºc Ä‘o thá»i gian\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    \n    print(f\"Thá»i gian thá»±c thi: {elapsed_time:.2f} giÃ¢y\")\n    \n    # LÆ°u thÃ´ng tin thá»i gian\n    with open(os.path.join(PATH_CONFIG['logs_dir'], 'execution_time.txt'), 'w') as f:\n        f.write(f\"Thá»i gian thá»±c thi: {elapsed_time:.2f} giÃ¢y\\n\")\n        f.write(f\"PhÆ°Æ¡ng phÃ¡p xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng: {args.imbalance_method}\\n\")\n\n# Cháº¡y chÆ°Æ¡ng trÃ¬nh\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:52:04.411590Z","iopub.execute_input":"2025-05-31T15:52:04.412322Z","iopub.status.idle":"2025-05-31T16:30:09.175485Z","shell.execute_reply.started":"2025-05-31T15:52:04.412296Z","shell.execute_reply":"2025-05-31T16:30:09.174946Z"}},"outputs":[{"name":"stdout","text":"Sá»­ dá»¥ng thiáº¿t bá»‹: cuda\nÄang Ä‘á»c dá»¯ liá»‡u...\nSá»‘ lÆ°á»£ng máº«u trong táº­p huáº¥n luyá»‡n trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': 5548\nSá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm Ä‘á»‹nh trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': 686\nSá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm tra trÆ°á»›c khi loáº¡i bá» nhÃ£n 'Other': 693\nSá»‘ lÆ°á»£ng máº«u trong táº­p huáº¥n luyá»‡n sau khi loáº¡i bá» nhÃ£n 'Other': 4527\nSá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm Ä‘á»‹nh sau khi loáº¡i bá» nhÃ£n 'Other': 545\nSá»‘ lÆ°á»£ng máº«u trong táº­p kiá»ƒm tra sau khi loáº¡i bá» nhÃ£n 'Other': 564\nÃp dá»¥ng weighted sampler cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng...\nÄang Ä‘á»c tá»« Ä‘iá»ƒn cáº£m xÃºc...\nÄang táº£i mÃ´ hÃ¬nh CafeBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/496 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d2023a2f5b4af5897de6638866f4e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573296dd1c164a92acea13f8a6c4836e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f9f7cf147b416b9969a532b3a2de3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e2c7a22f54456688562eb06a309ca5"}},"metadata":{}},{"name":"stderr","text":"2025-05-31 15:52:27.458350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748706747.699031      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748706747.768173      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08f739a54e564c1192b14f31d7e6ad22"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Äang táº¡o dataset...\nSá»­ dá»¥ng weighted sampler cho train dataloader\nÄang khá»Ÿi táº¡o mÃ´ hÃ¬nh...\nBáº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh...\nEpoch 1/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [04:56<00:00,  2.09s/it, loss=1.13] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.589]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2879\nVal Loss: 0.9810\nVal Macro F1: 0.5840\nVal Weighted F1: 0.6488\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.3908\nDisgust: 0.6017\nFear: 0.5714\nEnjoyment: 0.7750\nSadness: 0.6197\nSurprise: 0.5455\nÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t!\nEpoch 2/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:05<00:00,  2.15s/it, loss=0.638]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.114]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6896\nVal Loss: 0.9180\nVal Macro F1: 0.6295\nVal Weighted F1: 0.6847\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.4741\nDisgust: 0.5804\nFear: 0.5806\nEnjoyment: 0.8250\nSadness: 0.6705\nSurprise: 0.6462\nÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t!\nEpoch 3/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:05<00:00,  2.15s/it, loss=0.355]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.118]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4890\nVal Loss: 0.9527\nVal Macro F1: 0.6354\nVal Weighted F1: 0.6828\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.4874\nDisgust: 0.6133\nFear: 0.6269\nEnjoyment: 0.7893\nSadness: 0.6826\nSurprise: 0.6129\nÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t!\nEpoch 4/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:05<00:00,  2.15s/it, loss=0.283] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.088]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3939\nVal Loss: 0.9293\nVal Macro F1: 0.6545\nVal Weighted F1: 0.6943\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.5185\nDisgust: 0.6182\nFear: 0.6780\nEnjoyment: 0.8020\nSadness: 0.6704\nSurprise: 0.6400\nÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t!\nEpoch 5/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:05<00:00,  2.15s/it, loss=0.243] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.104]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2857\nVal Loss: 0.8892\nVal Macro F1: 0.6446\nVal Weighted F1: 0.7114\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.3836\nDisgust: 0.6817\nFear: 0.6780\nEnjoyment: 0.8317\nSadness: 0.6982\nSurprise: 0.5946\nEpoch 6/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:05<00:00,  2.15s/it, loss=0.396] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.63it/s, loss=0.0269]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2173\nVal Loss: 0.9901\nVal Macro F1: 0.6466\nVal Weighted F1: 0.7141\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.5094\nDisgust: 0.6565\nFear: 0.6032\nEnjoyment: 0.8518\nSadness: 0.6585\nSurprise: 0.6000\nEpoch 7/20\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [05:04<00:00,  2.15s/it, loss=0.0532]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.62it/s, loss=0.0677]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1653\nVal Loss: 1.0665\nVal Macro F1: 0.6448\nVal Weighted F1: 0.7113\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.4634\nDisgust: 0.6882\nFear: 0.6296\nEnjoyment: 0.8269\nSadness: 0.6739\nSurprise: 0.5867\nDá»«ng sá»›m sau 7 epochs vÃ¬ khÃ´ng cáº£i thiá»‡n!\nÄang táº£i mÃ´ hÃ¬nh tá»‘t nháº¥t...\nÄang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Macro F1-score: 0.6828\nWeighted F1-score: 0.7191\n\nF1-score cho tá»«ng lá»›p:\nAnger: 0.4416\nDisgust: 0.6621\nFear: 0.7742\nEnjoyment: 0.7898\nSadness: 0.7542\nSurprise: 0.6750\nThá»i gian thá»±c thi: 2275.24 giÃ¢y\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nimport pandas as pd\nimport re\nimport argparse\nimport emoji\n# Cáº¥u hÃ¬nh thiáº¿t bá»‹\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Cáº¥u hÃ¬nh tham sá»‘\nMAX_LEN = 128\n\n# HÃ m tiá»n xá»­ lÃ½ vÄƒn báº£n\ndef preprocess_text(text):\n    text = text.strip().lower()                      # Chuyá»ƒn vá» chá»¯ thÆ°á»ng\n    text = re.sub(r'\\s+', ' ', text)                 # XÃ³a khoáº£ng tráº¯ng thá»«a\n    text = re.sub(r'\\d+', '', text)                  # Loáº¡i bá» sá»‘\n    # Giá»¯ láº¡i emoji\n    emojis = ''.join(c for c in text if c in emoji.EMOJI_DATA)\n    # Loáº¡i kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, khoáº£ng tráº¯ng, dáº¥u cÃ¢u nháº¹\n    text = re.sub(r'[^\\w\\s\\.,!?]', '', text)\n    # GhÃ©p láº¡i vá»›i emoji náº¿u cáº§n\n    return text.strip() + ' ' + emojis if emojis else text.strip()\n\n# Äá»c tá»« Ä‘iá»ƒn cáº£m xÃºc VnEmoLex\ndef load_vnemolex():\n    vnemolex_df = pd.read_csv('/kaggle/input/uit-vsmec/VnEmoLex.csv')\n    \n    # Táº¡o tá»« Ä‘iá»ƒn cáº£m xÃºc\n    emotion_dict = {}\n    for _, row in vnemolex_df.iterrows():\n        word = row['Vietnamese']\n        emotions = {}\n        for emotion in ['Anger', 'Disgust', 'Fear', 'Enjoyment', 'Sadness', 'Surprise']:\n            if emotion in row and row[emotion] == 1:\n                emotions[emotion] = 1\n        if emotions:\n            emotion_dict[word] = emotions\n    \n    return emotion_dict\n\n# Táº¡o Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\ndef extract_lexicon_features(text, emotion_dict):\n    words = text.split()\n    features = {emotion: 0 for emotion in ['Anger', 'Disgust', 'Fear', 'Enjoyment', 'Sadness', 'Surprise']}\n    \n    for word in words:\n        if word in emotion_dict:\n            for emotion, value in emotion_dict[word].items():\n                features[emotion] += value\n    \n    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n    total = sum(features.values())\n    if total > 0:\n        for emotion in features:\n            features[emotion] /= total\n    \n    return list(features.values())\n\n# MÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc\nclass EmotionClassifier(nn.Module):\n    def __init__(self, bert_model, num_classes=6):\n        super(EmotionClassifier, self).__init__()\n        self.bert = bert_model\n        self.dropout = nn.Dropout(0.3)\n        \n        # KÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh BERT\n        self.bert_output_dim = self.bert.config.hidden_size\n        \n        # Sá»‘ Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\n        self.lexicon_features_dim = 6\n        \n        # Lá»›p káº¿t há»£p Ä‘áº·c trÆ°ng BERT vÃ  Ä‘áº·c trÆ°ng tá»« Ä‘iá»ƒn\n        self.feature_combiner = nn.Linear(self.bert_output_dim + self.lexicon_features_dim, 512)\n        \n        # Lá»›p phÃ¢n loáº¡i\n        self.classifier = nn.Linear(512, num_classes)\n        \n        # HÃ m kÃ­ch hoáº¡t\n        self.relu = nn.ReLU()\n    \n    def forward(self, input_ids, attention_mask, lexicon_features):\n        # Äáº§u ra tá»« mÃ´ hÃ¬nh BERT\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        \n        # Káº¿t há»£p Ä‘áº·c trÆ°ng BERT vÃ  Ä‘áº·c trÆ°ng tá»« Ä‘iá»ƒn\n        combined_features = torch.cat((pooled_output, lexicon_features), dim=1)\n        combined_features = self.feature_combiner(combined_features)\n        combined_features = self.relu(combined_features)\n        combined_features = self.dropout(combined_features)\n        \n        # PhÃ¢n loáº¡i\n        logits = self.classifier(combined_features)\n        \n        return logits\n\n# HÃ m dá»± Ä‘oÃ¡n cáº£m xÃºc cho vÄƒn báº£n má»›i\ndef predict_emotion(text, model, tokenizer, emotion_dict):\n    # Tiá»n xá»­ lÃ½ vÄƒn báº£n\n    text = preprocess_text(text)\n    \n    # Tokenize vÄƒn báº£n\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« tá»« Ä‘iá»ƒn cáº£m xÃºc\n    lexicon_features = extract_lexicon_features(text, emotion_dict)\n    \n    # ÄÆ°a dá»¯ liá»‡u lÃªn thiáº¿t bá»‹\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    lexicon_features = torch.tensor([lexicon_features], dtype=torch.float).to(device)\n    \n    # Dá»± Ä‘oÃ¡n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask, lexicon_features)\n        _, preds = torch.max(outputs, dim=1)\n    \n    # Ãnh xáº¡ sá»‘ sang nhÃ£n cáº£m xÃºc\n    emotion_map = {\n        0: 'Anger',\n        1: 'Disgust',\n        2: 'Fear',\n        3: 'Enjoyment',\n        4: 'Sadness',\n        5: 'Surprise'\n    }\n    \n    # Láº¥y xÃ¡c suáº¥t cho má»—i cáº£m xÃºc\n    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n    probs_dict = {emotion_map[i]: float(probabilities[0][i]) for i in range(6)}\n    \n    return emotion_map[preds.item()], probs_dict\n\n# HÃ m tÆ°Æ¡ng tÃ¡c vá»›i ngÆ°á»i dÃ¹ng\ndef get_user_input():\n    print(\"\\nChá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:\")\n    print(\"1. Nháº­p vÄƒn báº£n trá»±c tiáº¿p\")\n    print(\"2. Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n\")\n    print(\"3. ThoÃ¡t\")\n    \n    choice = input(\"Nháº­p lá»±a chá»n cá»§a báº¡n (1-3): \")\n    \n    if choice == '1':\n        text = input(\"\\nNháº­p vÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n cáº£m xÃºc: \")\n        return {'type': 'text', 'content': text}\n    elif choice == '2':\n        file_path = input(\"\\nNháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n: \")\n        return {'type': 'file', 'content': file_path}\n    elif choice == '3':\n        return {'type': 'exit'}\n    else:\n        print(\"Lá»±a chá»n khÃ´ng há»£p lá»‡. Vui lÃ²ng chá»n láº¡i.\")\n        return get_user_input()\n\n# HÃ m xá»­ lÃ½ dá»± Ä‘oÃ¡n cho vÄƒn báº£n\ndef process_text(text, model, tokenizer, emotion_dict):\n    emotion, probs = predict_emotion(text, model, tokenizer, emotion_dict)\n    print(f\"\\nVÄƒn báº£n: {text}\")\n    print(f\"Cáº£m xÃºc dá»± Ä‘oÃ¡n: {emotion}\")\n    print(\"\\nXÃ¡c suáº¥t cho má»—i cáº£m xÃºc:\")\n    for emotion, prob in sorted(probs.items(), key=lambda x: x[1], reverse=True):\n        print(f\"{emotion}: {prob:.4f}\")\n\n# HÃ m xá»­ lÃ½ dá»± Ä‘oÃ¡n cho file\ndef process_file(file_path, model, tokenizer, emotion_dict):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            texts = f.readlines()\n        \n        print(f\"\\nÄÃ£ Ä‘á»c {len(texts)} dÃ²ng tá»« file {file_path}\")\n        \n        results = []\n        for i, text in enumerate(texts):\n            text = text.strip()\n            if text:  # Bá» qua dÃ²ng trá»‘ng\n                emotion, probs = predict_emotion(text, model, tokenizer, emotion_dict)\n                results.append({\n                    'text': text,\n                    'emotion': emotion,\n                    'probabilities': probs\n                })\n                print(f\"DÃ²ng {i+1}: {emotion}\")\n        \n        # LÆ°u káº¿t quáº£ vÃ o file\n        output_file = file_path.rsplit('.', 1)[0] + '_predictions.csv'\n        df = pd.DataFrame(results)\n        df.to_csv(output_file, index=False)\n        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ o file {output_file}\")\n    \n    except Exception as e:\n        print(f\"Lá»—i khi Ä‘á»c file: {e}\")\n\n# HÃ m chÃ­nh\ndef main():\n    # PhÃ¢n tÃ­ch tham sá»‘ dÃ²ng lá»‡nh\n    parser = argparse.ArgumentParser(description='Dá»± Ä‘oÃ¡n cáº£m xÃºc cho vÄƒn báº£n tiáº¿ng Viá»‡t')\n    parser.add_argument('--model', type=str, default='/kaggle/working/models/best_model.pt', help='ÄÆ°á»ng dáº«n Ä‘áº¿n file mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n')\n    args = parser.parse_args([])\n    \n    # Äá»c tá»« Ä‘iá»ƒn cáº£m xÃºc\n    print(\"Äang Ä‘á»c tá»« Ä‘iá»ƒn cáº£m xÃºc...\")\n    emotion_dict = load_vnemolex()\n    \n    # Táº£i mÃ´ hÃ¬nh vÃ  tokenizer\n    print(\"Äang táº£i mÃ´ hÃ¬nh CafeBERT...\")\n    tokenizer = AutoTokenizer.from_pretrained('uitnlp/CafeBERT')\n    bert_model = AutoModel.from_pretrained('uitnlp/CafeBERT')\n    \n    # Táº¡o mÃ´ hÃ¬nh\n    print(\"Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh...\")\n    model = EmotionClassifier(bert_model)\n    model.to(device)\n    \n    # Táº£i trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n\n    print(f\"Äang táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n tá»« {args.model}...\")\n    model.load_state_dict(torch.load(args.model, map_location=device), strict=False)\n    print(\"MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng vá»›i strict=False Ä‘á»ƒ bá» qua cÃ¡c khÃ³a khÃ´ng khá»›p!\")\n    \n    # VÃ²ng láº·p tÆ°Æ¡ng tÃ¡c vá»›i ngÆ°á»i dÃ¹ng\n    while True:\n        user_input = get_user_input()\n        \n        if user_input['type'] == 'exit':\n            print(\"Táº¡m biá»‡t!\")\n            break\n        elif user_input['type'] == 'text':\n            process_text(user_input['content'], model, tokenizer, emotion_dict)\n        elif user_input['type'] == 'file':\n            process_file(user_input['content'], model, tokenizer, emotion_dict)\n\n# Cháº¡y chÆ°Æ¡ng trÃ¬nh\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T16:58:03.844043Z","iopub.execute_input":"2025-05-31T16:58:03.844265Z","iopub.status.idle":"2025-05-31T17:02:02.923323Z","shell.execute_reply.started":"2025-05-31T16:58:03.844250Z","shell.execute_reply":"2025-05-31T17:02:02.922669Z"}},"outputs":[{"name":"stdout","text":"Äang Ä‘á»c tá»« Ä‘iá»ƒn cáº£m xÃºc...\nÄang táº£i mÃ´ hÃ¬nh CafeBERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/CafeBERT and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh...\nÄang táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n tá»« /kaggle/working/models/best_model.pt...\nMÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng vá»›i strict=False Ä‘á»ƒ bá» qua cÃ¡c khÃ³a khÃ´ng khá»›p!\n\nChá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:\n1. Nháº­p vÄƒn báº£n trá»±c tiáº¿p\n2. Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n\n3. ThoÃ¡t\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nháº­p lá»±a chá»n cá»§a báº¡n (1-3):  1\n\nNháº­p vÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n cáº£m xÃºc:  Khi báº¡n nghÄ© ráº±ng cuá»™c sá»‘ng cá»§a báº¡n lÃ  hoÃ n háº£o, khÃ´ng cÃ²n má»¥c Ä‘Ã­ch lá»›n lao nÃ o ná»¯a. NÃ³ cÃ³ nghÄ©a lÃ  cuá»™c sá»‘ng Ä‘Ã£ máº¥t ráº¥t nhiá»u Ã½ nghÄ©a.\n"},{"name":"stdout","text":"\nVÄƒn báº£n: Khi báº¡n nghÄ© ráº±ng cuá»™c sá»‘ng cá»§a báº¡n lÃ  hoÃ n háº£o, khÃ´ng cÃ²n má»¥c Ä‘Ã­ch lá»›n lao nÃ o ná»¯a. NÃ³ cÃ³ nghÄ©a lÃ  cuá»™c sá»‘ng Ä‘Ã£ máº¥t ráº¥t nhiá»u Ã½ nghÄ©a.\nCáº£m xÃºc dá»± Ä‘oÃ¡n: Sadness\n\nXÃ¡c suáº¥t cho má»—i cáº£m xÃºc:\nSadness: 0.7884\nDisgust: 0.1625\nEnjoyment: 0.0290\nFear: 0.0116\nAnger: 0.0057\nSurprise: 0.0028\n\nChá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:\n1. Nháº­p vÄƒn báº£n trá»±c tiáº¿p\n2. Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n\n3. ThoÃ¡t\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nháº­p lá»±a chá»n cá»§a báº¡n (1-3):  1\n\nNháº­p vÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n cáº£m xÃºc:  Äá»«ng bao giá» tá»« bá» Æ°á»›c mÆ¡ cá»§a mÃ¬nh. HÃ£y táº¯t bÃ¡o thá»©c vÃ  lÄƒn ra ngá»§ tiáº¿p\n"},{"name":"stdout","text":"\nVÄƒn báº£n: Äá»«ng bao giá» tá»« bá» Æ°á»›c mÆ¡ cá»§a mÃ¬nh. HÃ£y táº¯t bÃ¡o thá»©c vÃ  lÄƒn ra ngá»§ tiáº¿p\nCáº£m xÃºc dá»± Ä‘oÃ¡n: Enjoyment\n\nXÃ¡c suáº¥t cho má»—i cáº£m xÃºc:\nEnjoyment: 0.4735\nSadness: 0.3993\nDisgust: 0.1011\nFear: 0.0189\nAnger: 0.0045\nSurprise: 0.0028\n\nChá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:\n1. Nháº­p vÄƒn báº£n trá»±c tiáº¿p\n2. Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n\n3. ThoÃ¡t\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nháº­p lá»±a chá»n cá»§a báº¡n (1-3):  1\n\nNháº­p vÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n cáº£m xÃºc:  Tui tao khÃ´ng ngá» Ä‘Æ°á»£c Ä‘Ã¢y lÃ  trÆ°á»ng tui\n"},{"name":"stdout","text":"\nVÄƒn báº£n: Tui tao khÃ´ng ngá» Ä‘Æ°á»£c Ä‘Ã¢y lÃ  trÆ°á»ng tui\nCáº£m xÃºc dá»± Ä‘oÃ¡n: Surprise\n\nXÃ¡c suáº¥t cho má»—i cáº£m xÃºc:\nSurprise: 0.9824\nEnjoyment: 0.0104\nSadness: 0.0024\nDisgust: 0.0023\nFear: 0.0015\nAnger: 0.0010\n\nChá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:\n1. Nháº­p vÄƒn báº£n trá»±c tiáº¿p\n2. Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file vÄƒn báº£n\n3. ThoÃ¡t\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nháº­p lá»±a chá»n cá»§a báº¡n (1-3):  3\n"},{"name":"stdout","text":"Táº¡m biá»‡t!\n","output_type":"stream"}],"execution_count":15}]}